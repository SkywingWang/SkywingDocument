技术选型：
速成版 hdfs，hive，hbase做存储，hive，mapreduce，spark分析引擎，PowerBI做报表，Azkaban做调度。
托管阿里云 Maxcompute Blink

学习路径：
### 大数据计算
#### 实践场景
搭建一套开源的Hadoop集群，在上面操作HDFS，Hive，Spark，HBase等各种组件。实践场景，从MySQL数据库把两张表导入到Hadoop，然后通过Hive进行计算，结果数据同步回MySQL数据库。
#### 同步工具的选择
待选Sqoop和DataX。
#### 数据加载方式
Hive的底层是HDFS，简单说是个文件，Hive只是映射过去，通过类SQL语言实现计算。你可以直接通过Hive接口(三种方式)建内部表，Sqoop和DataX都支持直接同步到Hive中。
#### 转化方式
Hive不支持存储，不支持update，所以可以进行两张表数据聚合后数据插入另一张表中，再同步回MySQL。
#### 流程如何串起来
可以通过Linux的Shell脚本进行串联，数据同步，数据转化，数据导出。
#### 如何启动流程
所有任务封装到sh脚本里，可以利用Linux的crontab进行定时调度。

### 数据仓库
#### 初步学习
初步接触数据仓库时，建议先看维度模型，了解什么是事实表，什么是维度表。做一张事实表，定义哪些是维度、哪些是度量，然后通过SQL进行查询。有了基本概念后，可以再学习深度的内容，例如星型模型、雪花模型。
进阶则可以学习维度建模：选择业务过程 - 声明粒度 - 确定维度 - 确定事实。
#### 步入设计
首先要了解数据仓库的分层，然后了解事实表的类型（事物、周期快照、累计快照）、维度表的类型（普通维度、缓慢变化维度）、总线矩阵、数据立方体等。
#### 高阶学习
维度建模实践后，发现维度建模的不足，通读并理解数据仓库之父Bill Inmon的范式建模和Kimball大师的维度建模，两者的建模各有优势，可以取长补短。
#### 解决业务问题
数据仓库的规划也可以自顶向下，采用Inmon的思想，开发和建模规范也要考虑全局，而在实施中可以采用维度建模，自底向上，采用Kimbal思想，落地快，迭代快。

### ELT技术
传统的ETL(Extract-Transofrm-Load)是把T的部分放在中间的，在大数据环境下，会把T放在后面，从ETL向ELT进行演变。原因也很简单，这样可以充分利用大数据环境T的能力。
E(Extract，抽取)和L(Load，装载)的优化需要懂源头和目标数据库(数据仓库)的特点，需要根据情况进行优化。T(Transform，转化)部分要理解底层技术原理，进行优化。
ELT的注意点总结如下：
时效性：必须在规定时间内跑完数据，跑出结果。
准确性：数据计算结果必须准确。
容错性：ELT可以支持重跑、补数等功能。
前瞻性：及时告警和预警功能，提前处理问题。

### 总结
还有两个加分项，应用系统和算法
一个应用系统一般会有前端、后端和数据库，怎么保持一个系统的稳定。要对高可用、负载均衡、安全有深刻的认识，需要考虑到应用、数据库、其他中间件。
要了解且会使用算法，机器学习算法，理解下预测、分类的概念。